{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de51a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/shared/.cache/huggingface\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/shared/.cache/huggingface/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f2a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    default_data_collator\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ✅ Step 1: Load soft labels from file\n",
    "with open(\"soft_labels_finetuned_biogpt.json\", \"r\") as f:\n",
    "    soft_dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38089dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"  # token\n",
    "api = HfApi(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d310846d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'michiyasunaga/BioLinkBERT-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'michiyasunaga/BioLinkBERT-large' is the correct path to a directory containing all relevant files for a RobertaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification\n\u001b[32m      5\u001b[39m student_model_id = \u001b[33m\"\u001b[39m\u001b[33mmichiyasunaga/BioLinkBERT-large\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m student_tokenizer = \u001b[43mRobertaTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m student_model = RobertaForSequenceClassification.from_pretrained(student_model_id, num_labels=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/shared/llm-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2046\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2044\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2045\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files.values()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2046\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   2047\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load tokenizer for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load it from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2048\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the same name. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2049\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOtherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a directory \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2050\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokenizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2051\u001b[39m     )\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files.items():\n\u001b[32m   2054\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[31mOSError\u001b[39m: Can't load tokenizer for 'michiyasunaga/BioLinkBERT-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'michiyasunaga/BioLinkBERT-large' is the correct path to a directory containing all relevant files for a RobertaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# ✅ Step 2: Load PubMedBERT tokenizer and model with manual BERT specification\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "student_model_id = \"michiyasunaga/BioLinkBERT-large\"\n",
    "student_tokenizer = RobertaTokenizer.from_pretrained(student_model_id)\n",
    "student_model = RobertaForSequenceClassification.from_pretrained(student_model_id, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37595c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1131e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 3: Convert soft dataset to HF Dataset and tokenize\n",
    "hf_dataset = Dataset.from_list(soft_dataset)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = student_tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = torch.tensor(example[\"soft_label\"], dtype=torch.float)\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, remove_columns=[\"input_text\", \"soft_label\", \"gold_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff501c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 4: Build DataLoader\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 5: Training config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model.to(device)\n",
    "\n",
    "optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.KLDivLoss(reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 6: Distillation training loop\n",
    "epochs = 3\n",
    "student_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        loss = loss_fn(log_probs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1} - Avg Distillation Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f9dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63da82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
