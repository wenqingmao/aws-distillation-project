{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daabd37",
   "metadata": {},
   "source": [
    "## 4.4 Comparison against Fine-tuning with Gold Labels (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edeb36",
   "metadata": {},
   "source": [
    "### 4.4.1 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f79a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/shared/.cache/huggingface\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/shared/.cache/huggingface/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abe7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    default_data_collator\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd048ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7381a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"hf_token\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcebca",
   "metadata": {},
   "source": [
    "### 4.4.2 Load Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9f68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 2: Load PubMedBERT tokenizer and model with manual BERT specification\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "# student_model_id = \"michiyasunaga/BioLinkBERT-base\"\n",
    "student_model_id = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "config = BertConfig.from_pretrained(student_model_id)\n",
    "config.num_labels = 3\n",
    "config.model_type = \"bert\" \n",
    "\n",
    "student_tokenizer = BertTokenizer.from_pretrained(student_model_id)\n",
    "student_model = BertForSequenceClassification.from_pretrained(student_model_id, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829b29b",
   "metadata": {},
   "source": [
    "### 4.4.3 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f839dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 0: Load soft labels from file\n",
    "with open(\"soft_labels_from_finetuned_biogpt.json\", \"r\") as f:\n",
    "    soft_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57a0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 1. Create a Dataset only with gold_index (teacher soft labels excluded)\n",
    "\n",
    "# Only keep input_text and gold_index from the original soft_dataset \n",
    "pure_gold_dataset = [{\"input_text\": item[\"input_text\"], \"gold_index\": item[\"gold_index\"]} for item in soft_dataset]\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "hf_dataset_gold = Dataset.from_list(pure_gold_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fda084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f698dd71e92d48b4b2d4c9fc090baf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ 2. Tokenize (soft labels excluded)\n",
    "def tokenize_gold(example):\n",
    "    tokens = student_tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"gold_index\"] = example[\"gold_index\"]\n",
    "    tokens[\"input_length\"] = len(student_tokenizer.tokenize(example[\"input_text\"]))\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset_gold = hf_dataset_gold.map(tokenize_gold, remove_columns=[\"input_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b3e3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf05bb98a2c649f086db32d6b524505b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop examples with input length greater than 512\n",
    "tokenized_dataset_gold = tokenized_dataset_gold.filter(lambda example: example[\"input_length\"] <= 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88614e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(974, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_gold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3f320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 3. Train Validation Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_list = tokenized_dataset_gold.to_list()\n",
    "train_list, val_list = train_test_split(tokenized_list, test_size=0.5, random_state=401)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_list)\n",
    "val_dataset = Dataset.from_list(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9aec2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 4. Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=default_data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36ca6a",
   "metadata": {},
   "source": [
    "### 4.4.4 Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d896798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 5. Define Metrics (Only CrossEntropy Loss)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model.to(device)\n",
    "\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "loss_fn_ce = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742d604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1_scores = []\n",
    "\n",
    "epochs = 50\n",
    "best_val_loss = 100\n",
    "best_accuracy = 0\n",
    "patience = 30\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd676b",
   "metadata": {},
   "source": [
    "### 4.4.5 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9788ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 0x0s, 0x1s, 487x2s\n",
      "Epoch 1 - Train Loss: 0.9771, Val Loss: 0.9188, Accuracy: 0.5626, Precision: 0.3166, Recall: 0.5626, F1-Score: 0.4052\n",
      "New best val_loss: 0.9187948299992469 New best accuracy: 0.5626283367556468\n",
      "Predictions: 1x0s, 0x1s, 486x2s\n",
      "Epoch 2 - Train Loss: 0.9612, Val Loss: 0.9195, Accuracy: 0.5606, Precision: 0.3160, Recall: 0.5606, F1-Score: 0.4042\n",
      "Predictions: 0x0s, 0x1s, 487x2s\n",
      "Epoch 3 - Train Loss: 0.9392, Val Loss: 0.9093, Accuracy: 0.5626, Precision: 0.3166, Recall: 0.5626, F1-Score: 0.4052\n",
      "New best val_loss: 0.9093194892329555 New best accuracy: 0.5626283367556468\n",
      "Predictions: 19x0s, 0x1s, 468x2s\n",
      "Epoch 4 - Train Loss: 0.9250, Val Loss: 0.9003, Accuracy: 0.5729, Precision: 0.5183, Recall: 0.5729, F1-Score: 0.4469\n",
      "New best val_loss: 0.900339291941735 New best accuracy: 0.5728952772073922\n",
      "Predictions: 32x0s, 0x1s, 455x2s\n",
      "Epoch 5 - Train Loss: 0.8786, Val Loss: 0.9005, Accuracy: 0.5749, Precision: 0.5052, Recall: 0.5749, F1-Score: 0.4644\n",
      "Predictions: 211x0s, 0x1s, 276x2s\n",
      "Epoch 6 - Train Loss: 0.8097, Val Loss: 0.8957, Accuracy: 0.5811, Precision: 0.5332, Recall: 0.5811, F1-Score: 0.5544\n",
      "New best val_loss: 0.8956644631201222 New best accuracy: 0.5811088295687885\n",
      "Predictions: 188x0s, 0x1s, 299x2s\n",
      "Epoch 7 - Train Loss: 0.6540, Val Loss: 0.8800, Accuracy: 0.6140, Precision: 0.5548, Recall: 0.6140, F1-Score: 0.5828\n",
      "New best val_loss: 0.8800069209068052 New best accuracy: 0.6139630390143738\n",
      "Predictions: 100x0s, 5x1s, 382x2s\n",
      "Epoch 8 - Train Loss: 0.4795, Val Loss: 0.9575, Accuracy: 0.6407, Precision: 0.5840, Recall: 0.6407, F1-Score: 0.5899\n",
      "Predictions: 200x0s, 12x1s, 275x2s\n",
      "Epoch 9 - Train Loss: 0.3925, Val Loss: 0.9392, Accuracy: 0.6427, Precision: 0.6191, Recall: 0.6427, F1-Score: 0.6234\n",
      "Predictions: 254x0s, 17x1s, 216x2s\n",
      "Epoch 10 - Train Loss: 0.2248, Val Loss: 1.1444, Accuracy: 0.6140, Precision: 0.6206, Recall: 0.6140, F1-Score: 0.5992\n",
      "Predictions: 219x0s, 40x1s, 228x2s\n",
      "Epoch 11 - Train Loss: 0.1485, Val Loss: 1.1441, Accuracy: 0.6078, Precision: 0.6199, Recall: 0.6078, F1-Score: 0.6063\n",
      "Predictions: 239x0s, 50x1s, 198x2s\n",
      "Epoch 12 - Train Loss: 0.1010, Val Loss: 1.3437, Accuracy: 0.6016, Precision: 0.6382, Recall: 0.6016, F1-Score: 0.6026\n",
      "Predictions: 211x0s, 53x1s, 223x2s\n",
      "Epoch 13 - Train Loss: 0.0625, Val Loss: 1.3611, Accuracy: 0.6016, Precision: 0.6250, Recall: 0.6016, F1-Score: 0.6061\n",
      "Predictions: 219x0s, 54x1s, 214x2s\n",
      "Epoch 14 - Train Loss: 0.0392, Val Loss: 1.4433, Accuracy: 0.6016, Precision: 0.6311, Recall: 0.6016, F1-Score: 0.6062\n",
      "Predictions: 231x0s, 39x1s, 217x2s\n",
      "Epoch 15 - Train Loss: 0.0265, Val Loss: 1.5037, Accuracy: 0.6140, Precision: 0.6329, Recall: 0.6140, F1-Score: 0.6119\n",
      "Predictions: 240x0s, 69x1s, 178x2s\n",
      "Epoch 16 - Train Loss: 0.0204, Val Loss: 1.7707, Accuracy: 0.5585, Precision: 0.6220, Recall: 0.5585, F1-Score: 0.5662\n",
      "Predictions: 242x0s, 58x1s, 187x2s\n",
      "Epoch 17 - Train Loss: 0.0176, Val Loss: 1.7844, Accuracy: 0.5749, Precision: 0.6247, Recall: 0.5749, F1-Score: 0.5789\n",
      "Predictions: 234x0s, 40x1s, 213x2s\n",
      "Epoch 18 - Train Loss: 0.0124, Val Loss: 1.7647, Accuracy: 0.6119, Precision: 0.6319, Recall: 0.6119, F1-Score: 0.6091\n",
      "Predictions: 247x0s, 46x1s, 194x2s\n",
      "Epoch 19 - Train Loss: 0.0102, Val Loss: 1.8654, Accuracy: 0.5852, Precision: 0.6218, Recall: 0.5852, F1-Score: 0.5840\n",
      "Predictions: 242x0s, 40x1s, 205x2s\n",
      "Epoch 20 - Train Loss: 0.0082, Val Loss: 1.8566, Accuracy: 0.6016, Precision: 0.6277, Recall: 0.6016, F1-Score: 0.5987\n",
      "Predictions: 256x0s, 39x1s, 192x2s\n",
      "Epoch 21 - Train Loss: 0.0074, Val Loss: 1.9199, Accuracy: 0.5749, Precision: 0.6108, Recall: 0.5749, F1-Score: 0.5714\n",
      "Predictions: 176x0s, 51x1s, 260x2s\n",
      "Epoch 22 - Train Loss: 0.0068, Val Loss: 1.7293, Accuracy: 0.6099, Precision: 0.6167, Recall: 0.6099, F1-Score: 0.6128\n",
      "Predictions: 211x0s, 39x1s, 237x2s\n",
      "Epoch 23 - Train Loss: 0.0072, Val Loss: 1.7835, Accuracy: 0.6263, Precision: 0.6320, Recall: 0.6263, F1-Score: 0.6236\n",
      "Predictions: 233x0s, 57x1s, 197x2s\n",
      "Epoch 24 - Train Loss: 0.0062, Val Loss: 1.9611, Accuracy: 0.5893, Precision: 0.6323, Recall: 0.5893, F1-Score: 0.5942\n",
      "Predictions: 216x0s, 39x1s, 232x2s\n",
      "Epoch 25 - Train Loss: 0.0043, Val Loss: 1.8740, Accuracy: 0.6181, Precision: 0.6261, Recall: 0.6181, F1-Score: 0.6153\n",
      "Predictions: 256x0s, 23x1s, 208x2s\n",
      "Epoch 26 - Train Loss: 0.0053, Val Loss: 2.0137, Accuracy: 0.6099, Precision: 0.6230, Recall: 0.6099, F1-Score: 0.5975\n",
      "Predictions: 221x0s, 49x1s, 217x2s\n",
      "Epoch 27 - Train Loss: 0.0037, Val Loss: 1.9922, Accuracy: 0.6119, Precision: 0.6361, Recall: 0.6119, F1-Score: 0.6141\n",
      "Predictions: 216x0s, 54x1s, 217x2s\n",
      "Epoch 28 - Train Loss: 0.0036, Val Loss: 2.0037, Accuracy: 0.6016, Precision: 0.6304, Recall: 0.6016, F1-Score: 0.6069\n",
      "Predictions: 214x0s, 47x1s, 226x2s\n",
      "Epoch 29 - Train Loss: 0.0029, Val Loss: 1.9923, Accuracy: 0.6099, Precision: 0.6279, Recall: 0.6099, F1-Score: 0.6117\n",
      "Predictions: 219x0s, 44x1s, 224x2s\n",
      "Epoch 30 - Train Loss: 0.0027, Val Loss: 2.0090, Accuracy: 0.6119, Precision: 0.6289, Recall: 0.6119, F1-Score: 0.6121\n",
      "Predictions: 215x0s, 45x1s, 227x2s\n",
      "Epoch 31 - Train Loss: 0.0026, Val Loss: 2.0286, Accuracy: 0.6099, Precision: 0.6258, Recall: 0.6099, F1-Score: 0.6106\n",
      "Predictions: 204x0s, 48x1s, 235x2s\n",
      "Epoch 32 - Train Loss: 0.0024, Val Loss: 2.0304, Accuracy: 0.6037, Precision: 0.6175, Recall: 0.6037, F1-Score: 0.6060\n",
      "Predictions: 221x0s, 41x1s, 225x2s\n",
      "Epoch 33 - Train Loss: 0.0025, Val Loss: 2.0741, Accuracy: 0.6140, Precision: 0.6290, Recall: 0.6140, F1-Score: 0.6130\n",
      "Predictions: 225x0s, 41x1s, 221x2s\n",
      "Epoch 34 - Train Loss: 0.0023, Val Loss: 2.1074, Accuracy: 0.6140, Precision: 0.6312, Recall: 0.6140, F1-Score: 0.6129\n",
      "Predictions: 221x0s, 44x1s, 222x2s\n",
      "Epoch 35 - Train Loss: 0.0021, Val Loss: 2.1174, Accuracy: 0.6160, Precision: 0.6341, Recall: 0.6160, F1-Score: 0.6161\n",
      "Predictions: 215x0s, 52x1s, 220x2s\n",
      "Epoch 36 - Train Loss: 0.0021, Val Loss: 2.1440, Accuracy: 0.6078, Precision: 0.6326, Recall: 0.6078, F1-Score: 0.6118\n",
      "Predictions: 228x0s, 50x1s, 209x2s\n",
      "Epoch 37 - Train Loss: 0.0020, Val Loss: 2.2075, Accuracy: 0.5975, Precision: 0.6276, Recall: 0.5975, F1-Score: 0.6000\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 6: Gold-only training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    student_model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        hard_labels = batch[\"gold_index\"].to(device)\n",
    "\n",
    "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn_ce(logits, hard_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    student_model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_hard_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            hard_labels = batch[\"gold_index\"].to(device)\n",
    "\n",
    "            logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss = loss_fn_ce(logits, hard_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (predictions == hard_labels).sum().item()\n",
    "            total += hard_labels.size(0)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_hard_labels.extend(hard_labels.cpu().numpy())\n",
    "\n",
    "    sum_0 = np.sum(np.array(all_predictions) == 0)\n",
    "    sum_1 = np.sum(np.array(all_predictions) == 1)\n",
    "    sum_2 = np.sum(np.array(all_predictions) == 2)\n",
    "    print(f\"Predictions: {sum_0}x0s, {sum_1}x1s, {sum_2}x2s\")\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    precision = precision_score(all_hard_labels, all_predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(all_hard_labels, all_predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(all_hard_labels, all_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "    val_f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            # Save the best gold-only model\n",
    "            #output_dir = \"/home/shared/models/student_goldfinetune\"\n",
    "            #student_model.save_pretrained(output_dir)\n",
    "            #student_tokenizer.save_pretrained(output_dir)\n",
    "            #print(\"Best model saved! (Gold-only)\")\n",
    "            print(\"New best val_loss:\", best_val_loss, \"New best accuracy:\", best_accuracy)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
